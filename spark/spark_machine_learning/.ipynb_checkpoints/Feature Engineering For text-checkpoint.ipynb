{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.Builder().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre - processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdf = spark.createDataFrame([(0, 'I am learning Spark.')\n",
    "                                , (1, 'I love machine learning and deep learning.')\n",
    "                                , (2, 'I have completed master in big data analytics.')\n",
    "                                , (3, 'What about you my friend.')\n",
    "                                , (4, 'I need a job.')], ['id', 'Document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- Document: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I am learning Spark.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I love machine learning and deep learning.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I have completed master in big data analytics.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What about you my friend.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I need a job.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                        Document\n",
       "0   0                            I am learning Spark.\n",
       "1   1      I love machine learning and deep learning.\n",
       "2   2  I have completed master in big data analytics.\n",
       "3   3                       What about you my friend.\n",
       "4   4                                   I need a job."
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputdf.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------+-------------------------------------------------------+\n",
      "|id |Document                                      |words                                                  |\n",
      "+---+----------------------------------------------+-------------------------------------------------------+\n",
      "|0  |I am learning Spark.                          |[i, am, learning, spark.]                              |\n",
      "|1  |I love machine learning and deep learning.    |[i, love, machine, learning, and, deep, learning.]     |\n",
      "|2  |I have completed master in big data analytics.|[i, have, completed, master, in, big, data, analytics.]|\n",
      "|3  |What about you my friend.                     |[what, about, you, my, friend.]                        |\n",
      "|4  |I need a job.                                 |[i, need, a, job.]                                     |\n",
      "+---+----------------------------------------------+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"Document\", outputCol=\"words\")\n",
    "\n",
    "tokenizerdf = tokenizer.transform(inputdf)\n",
    "tokenizerdf.select('id', 'Document', 'words').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------+------------------------------------------------------+\n",
      "|id |Document                                      |words                                                 |\n",
      "+---+----------------------------------------------+------------------------------------------------------+\n",
      "|0  |I am learning Spark.                          |[i, am, learning, spark]                              |\n",
      "|1  |I love machine learning and deep learning.    |[i, love, machine, learning, and, deep, learning]     |\n",
      "|2  |I have completed master in big data analytics.|[i, have, completed, master, in, big, data, analytics]|\n",
      "|3  |What about you my friend.                     |[what, about, you, my, friend]                        |\n",
      "|4  |I need a job.                                 |[i, need, a, job]                                     |\n",
      "+---+----------------------------------------------+------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenizer = RegexTokenizer(inputCol = \"Document\", outputCol = \"words\", pattern = \"\\\\s+|,|\\\\.\")\n",
    "\n",
    "tokenizedDF = regexTokenizer.transform(inputdf)\n",
    "tokenizedDF.select('id', 'Document', 'words').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsRemover = StopWordsRemover(inputCol = \"words\", outputCol = \"words_filtered\")\n",
    "stopwords = stopwordsRemover.loadDefaultStopWords('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|            Document|               words|      words_filtered|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  0|I am learning Spark.|[i, am, learning,...|   [learning, spark]|\n",
      "|  1|I love machine le...|[i, love, machine...|[love, machine, l...|\n",
      "|  2|I have completed ...|[i, have, complet...|[completed, maste...|\n",
      "|  3|What about you my...|[what, about, you...|            [friend]|\n",
      "|  4|       I need a job.|   [i, need, a, job]|         [need, job]|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleandf = stopwordsRemover.transform(tokenizedDF)\n",
    "cleandf.show(truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            Document|             bigrams|\n",
      "+--------------------+--------------------+\n",
      "|I am learning Spark.|[i am, am learnin...|\n",
      "|I love machine le...|[i love, love mac...|\n",
      "|I have completed ...|[i have, have com...|\n",
      "|What about you my...|[what about, abou...|\n",
      "|       I need a job.|[i need, need a, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram = NGram(n = 2, inputCol = \"words\", outputCol = \"bigrams\")  # bigrams\n",
    "ngramDF = ngram.transform(cleandf)\n",
    "ngramDF.select('Document', 'bigrams').show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            Document|            trigrams|\n",
      "+--------------------+--------------------+\n",
      "|I am learning Spark.|[i am learning, a...|\n",
      "|I love machine le...|[i love machine, ...|\n",
      "|I have completed ...|[i have completed...|\n",
      "|What about you my...|[what about you, ...|\n",
      "|       I need a job.|[i need a, need a...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram = NGram(n = 3, inputCol = \"words\", outputCol = \"trigrams\")  # trigrams\n",
    "ngramDF = ngram.transform(ngramDF)\n",
    "ngramDF.select('Document', 'trigrams').show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Document='I am learning Spark.', bigrams=['i am', 'am learning', 'learning spark'], trigrams=['i am learning', 'am learning spark'])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngramDF.select('Document', 'bigrams', 'trigrams').collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizer = CountVectorizer(inputCol = \"words_filtered\", outputCol = \"features_tf\") \n",
    "model = countVectorizer.fit(cleandf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learning', 'spark', 'job', 'completed', 'machine', 'deep', 'big', 'need', 'master', 'love', 'data', 'analytics', 'friend']\n"
     ]
    }
   ],
   "source": [
    "print(model.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|            Document|               words|      words_filtered|         features_tf|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|I am learning Spark.|[i, am, learning,...|   [learning, spark]|(13,[0,1],[1.0,1.0])|\n",
      "|  1|I love machine le...|[i, love, machine...|[love, machine, l...|(13,[0,4,5,9],[2....|\n",
      "|  2|I have completed ...|[i, have, complet...|[completed, maste...|(13,[3,6,8,10,11]...|\n",
      "|  3|What about you my...|[what, about, you...|            [friend]|     (13,[12],[1.0])|\n",
      "|  4|       I need a job.|   [i, need, a, job]|         [need, job]|(13,[2,7],[1.0,1.0])|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleandf = model.transform(cleandf)\n",
    "cleandf.show(truncate =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(cleandf.collect()[0][\"features_tf\"].toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol = \"features_tf\", outputCol = \"features_tf_idf\")\n",
    "idfModel = idf.fit(cleandf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|            Document|               words|      words_filtered|         features_tf|     features_tf_idf|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|I am learning Spark.|[i, am, learning,...|   [learning, spark]|(13,[0,1],[1.0,1.0])|(13,[0,1],[0.6931...|\n",
      "|  1|I love machine le...|[i, love, machine...|[love, machine, l...|(13,[0,4,5,9],[2....|(13,[0,4,5,9],[1....|\n",
      "|  2|I have completed ...|[i, have, complet...|[completed, maste...|(13,[3,6,8,10,11]...|(13,[3,6,8,10,11]...|\n",
      "|  3|What about you my...|[what, about, you...|            [friend]|     (13,[12],[1.0])|(13,[12],[1.09861...|\n",
      "|  4|       I need a job.|   [i, need, a, job]|         [need, job]|(13,[2,7],[1.0,1.0])|(13,[2,7],[1.0986...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idfModel = idfModel.transform(cleandf)\n",
    "idfModel.show(truncate = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
